# Kafka Event Architecture Lab

ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ ê¸°ë°˜ ì•„í‚¤í…ì²˜ ì‹¤ìŠµ í”„ë¡œì íŠ¸ - Kafka Streamsë¥¼ í™œìš©í•œ ì£¼ë¬¸ ì²˜ë¦¬ ë° í†µê³„ ì‹œìŠ¤í…œ

## ëª©ì°¨

- [í”„ë¡œì íŠ¸ ê°œìš”](#í”„ë¡œì íŠ¸-ê°œìš”)
- [ì•„í‚¤í…ì²˜ ì„¤ëª…](#ì•„í‚¤í…ì²˜-ì„¤ëª…)
- [ğŸ“š Kafka Streams í•µì‹¬ ê°œë…](#-kafka-streams-í•µì‹¬-ê°œë…) â­ **ì¤‘ìš”**
- [ê¸°ìˆ  ìŠ¤íƒ](#ê¸°ìˆ -ìŠ¤íƒ)
- [ğŸ  ë¡œì»¬ ê°œë°œ í™˜ê²½ì—ì„œ ì‚¬ìš©í•˜ê¸°](#-ë¡œì»¬-ê°œë°œ-í™˜ê²½ì—ì„œ-ì‚¬ìš©í•˜ê¸°) â­ **ì¶”ì²œ**
- [ë¡œì»¬ í™˜ê²½ ì‹¤ìŠµ (ìƒì„¸)](#ë¡œì»¬-í™˜ê²½-ì‹¤ìŠµ)
- [AWS ë°°í¬ ê°€ì´ë“œ](#aws-ë°°í¬-ê°€ì´ë“œ)
- [API ëª…ì„¸](#api-ëª…ì„¸)
- [íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)

---

## í”„ë¡œì íŠ¸ ê°œìš”

ì´ í”„ë¡œì íŠ¸ëŠ” **ì´ë²¤íŠ¸ ê¸°ë°˜ ì•„í‚¤í…ì²˜(Event-Driven Architecture)**ì˜ í•µì‹¬ ê°œë…ì„ ì‹¤ìŠµí•˜ê¸° ìœ„í•œ ì™„ì „í•œ ë°ëª¨ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

### ì£¼ìš” ê¸°ëŠ¥

1. **ì£¼ë¬¸ ìƒì„± ë° ê´€ë¦¬** (Server1)
   - REST APIë¥¼ í†µí•œ ì£¼ë¬¸ ìƒì„±
   - PostgreSQLì— ì£¼ë¬¸ ë°ì´í„° ì €ì¥
   - Kafkaë¡œ ì£¼ë¬¸ ì´ë²¤íŠ¸ ë°œí–‰
   - ì›¹ UIë¥¼ í†µí•œ ëœë¤ ì£¼ë¬¸ ìƒì„± ë²„íŠ¼ ì œê³µ

2. **ì‹¤ì‹œê°„ í†µê³„ ì²˜ë¦¬** (Server2)
   - Kafka Streamsë¥¼ ì´ìš©í•œ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬
   - ì£¼ë¬¸ ìˆ˜ ë° ë§¤ì¶œ ì‹¤ì‹œê°„ ì§‘ê³„
   - ì§€ì—­ë³„ í†µê³„ ë¶„ì„
   - Stateful ì²˜ë¦¬ (RocksDB ìƒíƒœ ì €ì¥ì†Œ)

3. **ëŒ€ì‹œë³´ë“œ** (Server1)
   - KStream/KTable ì²˜ë¦¬ íë¦„ ì‹œê°í™”
   - ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ ëª¨ë‹ˆí„°ë§
   - ì§‘ê³„ ìƒíƒœ ë³€ê²½ ì´ë ¥ ì¶”ì 
   - í†µê³„ ì‹œê°í™” ë° ìë™ ê°±ì‹ 
   - ì›¹ UI ëœë¤ ì£¼ë¬¸ ìƒì„± ë²„íŠ¼

### í•™ìŠµ ëª©í‘œ

- Kafka Producer/Consumer íŒ¨í„´ ì´í•´
- Kafka Streams ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ êµ¬í˜„
- ì´ë²¤íŠ¸ ê¸°ë°˜ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ê°„ í†µì‹ 
- Dockerë¥¼ í™œìš©í•œ ì»¨í…Œì´ë„ˆ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
- AWS í™˜ê²½ì—ì„œì˜ ì‹¤ì œ ë°°í¬

---

## ì•„í‚¤í…ì²˜ ì„¤ëª…

### ì‹œìŠ¤í…œ êµ¬ì„±ë„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ì‚¬ìš©ì / í´ë¼ì´ì–¸íŠ¸                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Server1 (8080)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  REST API (Express.js)                                â”‚  â”‚
â”‚  â”‚  â€¢ POST /orders     - ì£¼ë¬¸ ìƒì„±                        â”‚  â”‚
â”‚  â”‚  â€¢ GET /orders      - ì£¼ë¬¸ ì¡°íšŒ                        â”‚  â”‚
â”‚  â”‚  â€¢ GET /stats       - í†µê³„ ì¡°íšŒ                        â”‚  â”‚
â”‚  â”‚  â€¢ GET /dashboard   - ëŒ€ì‹œë³´ë“œ UI                      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                              â”‚                               â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚      â–¼                      â–¼                      â–¼        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  DB    â”‚          â”‚   Kafka    â”‚         â”‚  Kafka   â”‚   â”‚
â”‚  â”‚ Writer â”‚          â”‚  Producer  â”‚         â”‚ Consumer â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                      â”‚                      â”‚
       â–¼                      â”‚                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚                      â”‚
â”‚ PostgreSQL â”‚                â”‚                      â”‚
â”‚  Database  â”‚                â”‚                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚                      â”‚
                              â–¼                      â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
                    â”‚   Kafka Broker      â”‚          â”‚
                    â”‚   (Port 9092)       â”‚          â”‚
                    â”‚                     â”‚          â”‚
                    â”‚  Topics:            â”‚          â”‚
                    â”‚  â€¢ orders           â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚  â€¢ order-stats      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Server2                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Kafka Streams Application (Java)                     â”‚  â”‚
â”‚  â”‚                                                        â”‚  â”‚
â”‚  â”‚  1. orders í† í”½ êµ¬ë…                                   â”‚  â”‚
â”‚  â”‚  2. ì£¼ë¬¸ ë°ì´í„° ì§‘ê³„:                                  â”‚  â”‚
â”‚  â”‚     â€¢ ì´ ì£¼ë¬¸ ìˆ˜                                       â”‚  â”‚
â”‚  â”‚     â€¢ ì´ ë§¤ì¶œ                                          â”‚  â”‚
â”‚  â”‚     â€¢ ì§€ì—­ë³„ í†µê³„                                      â”‚  â”‚
â”‚  â”‚  3. order-stats í† í”½ìœ¼ë¡œ ê²°ê³¼ ë°œí–‰                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ë°ì´í„° íë¦„ (ì´ë²¤íŠ¸ ì•„í‚¤í…ì²˜)

```
1. ì£¼ë¬¸ ìƒì„±
   ì‚¬ìš©ì â†’ POST /orders â†’ Server1

2. ë°ì´í„° ì €ì¥ ë° ì´ë²¤íŠ¸ ë°œí–‰
   Server1 â†’ PostgreSQL (ì£¼ë¬¸ ì €ì¥)
   Server1 â†’ Kafka (orders í† í”½) (ì´ë²¤íŠ¸ ë°œí–‰)

3. ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬
   Kafka (orders í† í”½) â†’ Server2 (Kafka Streams)
   Server2 â†’ ì‹¤ì‹œê°„ ì§‘ê³„ ì²˜ë¦¬
   Server2 â†’ Kafka (order-stats í† í”½) (í†µê³„ ë°œí–‰)

4. ëŒ€ì‹œë³´ë“œ ê°±ì‹ 
   Kafka (order-stats í† í”½) â†’ Server1 (Consumer)
   Server1 â†’ ëŒ€ì‹œë³´ë“œ UI (ì‹¤ì‹œê°„ ê°±ì‹ )
```

### ì´ë²¤íŠ¸ ìŠ¤í‚¤ë§ˆ

#### orders í† í”½ (Input)

```json
{
  "order_id": "uuid-1234-5678",
  "user_id": "user-001",
  "store_id": "store-777",
  "region": "Seoul",
  "price": 15000,
  "status": "CREATED",
  "created_at": "2025-11-17T13:45:00Z"
}
```

#### order-stats í† í”½ (Output)

```json
{
  "totalOrders": 100,
  "totalSales": 1500000,
  "byRegion": {
    "Seoul": {
      "orders": 50,
      "sales": 750000
    },
    "Busan": {
      "orders": 30,
      "sales": 450000
    },
    "Incheon": {
      "orders": 20,
      "sales": 300000
    }
  },
  "lastUpdated": "2025-11-17T13:50:00Z"
}
```

---

## ğŸ“š Kafka Streams í•µì‹¬ ê°œë…

ì´ í”„ë¡œì íŠ¸ëŠ” Kafka Streamsì˜ í•µì‹¬ ê°œë…ë“¤ì„ ì‹¤ì œë¡œ êµ¬í˜„í•œ ì˜ˆì œì…ë‹ˆë‹¤.

### KStreamì´ë€?

**KStream**ì€ ë¬´í•œí•œ ë ˆì½”ë“œì˜ **ìŠ¤íŠ¸ë¦¼(Stream)**ì…ë‹ˆë‹¤.

#### íŠ¹ì§•
- **INSERT-ONLY**: ìƒˆë¡œìš´ ì´ë²¤íŠ¸ë§Œ ì¶”ê°€ë¨
- **Immutable**: ê¸°ì¡´ ë°ì´í„°ëŠ” ë³€ê²½ë˜ì§€ ì•ŠìŒ
- **ë¬´í•œ ìŠ¤íŠ¸ë¦¼**: ëì´ ì—†ëŠ” ì—°ì†ì ì¸ ë°ì´í„° íë¦„
- **ì´ë²¤íŠ¸ ë¡œê·¸**: ê° ë ˆì½”ë“œëŠ” ë…ë¦½ì ì¸ ì‚¬ì‹¤(fact)

#### ì˜ˆì‹œ
```
ì£¼ë¬¸ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼:
order-1: {user_id: "user-001", price: 15000} â† 15:00:01
order-2: {user_id: "user-002", price: 25000} â† 15:00:05
order-3: {user_id: "user-001", price: 10000} â† 15:00:10
...ê³„ì† ì¶”ê°€ë¨
```

#### ì´ í”„ë¡œì íŠ¸ì—ì„œì˜ ì‚¬ìš©
```java
// orders í† í”½ì—ì„œ KStream ìƒì„±
KStream<String, Order> ordersStream = builder.stream(
    "orders",
    Consumed.with(Serdes.String(), orderSerde)
);

// ê° ì£¼ë¬¸ ì´ë²¤íŠ¸ëŠ” ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬ë¨
ordersStream.foreach((key, order) -> {
    logger.info("ğŸ“¥ Received order: {}", order.getOrderId());
});
```

### KTableì´ë€?

**KTable**ì€ ë³€ê²½ ê°€ëŠ¥í•œ **ìƒíƒœ í…Œì´ë¸”(State Table)**ì…ë‹ˆë‹¤.

#### íŠ¹ì§•
- **UPDATE ê°€ëŠ¥**: ê°™ì€ í‚¤ì˜ ê°’ì´ ì—…ë°ì´íŠ¸ë¨
- **Mutable**: ìµœì‹  ê°’ìœ¼ë¡œ ë®ì–´ì”€
- **Changelog Stream**: ë³€ê²½ ì´ë ¥ì„ ì¶”ì 
- **í˜„ì¬ ìƒíƒœ**: ê° í‚¤ì— ëŒ€í•œ ìµœì‹  ê°’ë§Œ ìœ ì§€

#### ì˜ˆì‹œ
```
í†µê³„ í…Œì´ë¸” (Key: "global"):
15:00:01 â†’ {totalOrders: 1, totalSales: 15000}
15:00:05 â†’ {totalOrders: 2, totalSales: 40000}  â† ì—…ë°ì´íŠ¸ë¨
15:00:10 â†’ {totalOrders: 3, totalSales: 50000}  â† ì—…ë°ì´íŠ¸ë¨
```

#### ì´ í”„ë¡œì íŠ¸ì—ì„œì˜ ì‚¬ìš©
```java
// KStreamì„ ì§‘ê³„í•˜ì—¬ KTable ìƒì„±
KTable<String, OrderStats> statsTable = ordersStream
    .filter((key, order) -> order != null)
    .groupBy((key, order) -> "global")
    .aggregate(
        OrderStats::new,  // ì´ˆê¸°ê°’
        (key, order, stats) -> {
            // ìƒˆ ì£¼ë¬¸ë§ˆë‹¤ í†µê³„ ì—…ë°ì´íŠ¸
            stats.setTotalOrders(stats.getTotalOrders() + 1);
            stats.setTotalSales(stats.getTotalSales() + order.getPrice());
            return stats;
        },
        Materialized.with(Serdes.String(), statsSerde)
    );
```

### KStream vs KTable ë¹„êµ

| êµ¬ë¶„ | KStream | KTable |
|-----|---------|--------|
| **ë°ì´í„° ëª¨ë¸** | ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ | ìƒíƒœ í…Œì´ë¸” |
| **ì—°ì‚° íƒ€ì…** | INSERT only | INSERT, UPDATE, DELETE |
| **ì €ì¥ ë°©ì‹** | ëª¨ë“  ì´ë²¤íŠ¸ ë³´ê´€ | ìµœì‹  ê°’ë§Œ ë³´ê´€ |
| **ì‚¬ìš© ì‚¬ë¡€** | ì£¼ë¬¸ ë‚´ì—­, í´ë¦­ ë¡œê·¸ | ì‚¬ìš©ì í”„ë¡œí•„, ì¬ê³  ìˆ˜ëŸ‰ |
| **DB ë¹„ìœ ** | Transaction Log | Table |
| **ì˜ˆì‹œ** | "ì£¼ë¬¸ì´ ìƒì„±ë¨" | "í˜„ì¬ ì¬ê³ ëŠ” 10ê°œ" |

### Stateless vs Stateful ì—°ì‚°

#### Stateless ì—°ì‚° (ë¬´ìƒíƒœ)
ì´ì „ ë°ì´í„°ë¥¼ ê¸°ì–µí•  í•„ìš”ê°€ ì—†ëŠ” ì—°ì‚°

```java
// filter: ì¡°ê±´ì— ë§ëŠ” ë ˆì½”ë“œë§Œ í†µê³¼
ordersStream.filter((key, order) -> order.getPrice() > 10000)

// map: ê° ë ˆì½”ë“œë¥¼ ë³€í™˜
ordersStream.map((key, order) ->
    KeyValue.pair(order.getRegion(), order)
)

// flatMap: í•˜ë‚˜ì˜ ë ˆì½”ë“œë¥¼ ì—¬ëŸ¬ ê°œë¡œ ë³€í™˜
ordersStream.flatMap((key, order) ->
    Arrays.asList(
        KeyValue.pair(order.getUserId(), order),
        KeyValue.pair(order.getStoreId(), order)
    )
)
```

#### Stateful ì—°ì‚° (ìƒíƒœ ìœ ì§€)
ì´ì „ ë°ì´í„°ë¥¼ ê¸°ì–µí•˜ê³  ìƒíƒœë¥¼ ìœ ì§€í•˜ëŠ” ì—°ì‚°

```java
// groupBy: í‚¤ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™” (ë¦¬íŒŒí‹°ì…”ë‹)
ordersStream.groupBy((key, order) -> order.getRegion())

// aggregate: ì§‘ê³„ ì—°ì‚° (KTable ìƒì„±)
.aggregate(
    () -> new RegionStats(),      // ì´ˆê¸°ê°’
    (key, order, stats) -> {      // Aggregator
        stats.increment();
        return stats;
    },
    Materialized.with(...)         // ìƒíƒœ ì €ì¥ì†Œ ì„¤ì •
)

// reduce: ê°’ì„ ì¶•ì†Œ (KTable ìƒì„±)
.reduce((oldValue, newValue) -> oldValue + newValue)

// count: ê°œìˆ˜ ì„¸ê¸° (KTable ìƒì„±)
.count()
```

### ì£¼ìš” DSL ì—°ì‚°ì

#### 1. filter / filterNot
ì¡°ê±´ì— ë§ëŠ”/ë§ì§€ ì•ŠëŠ” ë ˆì½”ë“œë§Œ í†µê³¼
```java
ordersStream
    .filter((key, order) -> order != null)
    .filterNot((key, order) -> order.getPrice() == 0)
```

#### 2. map / mapValues
ë ˆì½”ë“œë¥¼ ë³€í™˜ (í‚¤-ê°’ ìŒ ë³€í™˜ / ê°’ë§Œ ë³€í™˜)
```java
// map: í‚¤ì™€ ê°’ ëª¨ë‘ ë³€ê²½
ordersStream.map((key, order) ->
    KeyValue.pair(order.getRegion(), order.getPrice())
)

// mapValues: ê°’ë§Œ ë³€ê²½ (í‚¤ëŠ” ìœ ì§€)
ordersStream.mapValues(order -> order.getPrice())
```

#### 3. groupBy / groupByKey
ë°ì´í„°ë¥¼ ê·¸ë£¹í™”
```java
// groupBy: ìƒˆë¡œìš´ í‚¤ë¡œ ê·¸ë£¹í™” (ë¦¬íŒŒí‹°ì…”ë‹ ë°œìƒ)
ordersStream.groupBy((key, order) -> order.getRegion())

// groupByKey: í˜„ì¬ í‚¤ë¡œ ê·¸ë£¹í™” (ë¦¬íŒŒí‹°ì…”ë‹ ì—†ìŒ)
ordersStream.groupByKey()
```

#### 4. aggregate
ì§‘ê³„ ì—°ì‚° ìˆ˜í–‰
```java
ordersStream
    .groupBy((key, order) -> order.getRegion())
    .aggregate(
        RegionStats::new,              // Initializer
        (key, order, stats) -> {       // Aggregator
            stats.addOrder(order);
            return stats;
        },
        Materialized.<String, RegionStats, KeyValueStore<Bytes, byte[]>>as("region-stats-store")
            .withKeySerde(Serdes.String())
            .withValueSerde(regionStatsSerde)
    )
```

#### 5. join
ë‘ ìŠ¤íŠ¸ë¦¼/í…Œì´ë¸”ì„ ì¡°ì¸
```java
// KStream-KTable join
ordersStream.join(
    usersTable,                        // KTable
    (order, user) -> {                 // ValueJoiner
        order.setUserName(user.getName());
        return order;
    },
    Joined.with(Serdes.String(), orderSerde, userSerde)
)
```

#### 6. peek
ë ˆì½”ë“œë¥¼ ë³€ê²½í•˜ì§€ ì•Šê³  ë¶€ìˆ˜ íš¨ê³¼ë§Œ ìˆ˜í–‰ (ë””ë²„ê¹…ìš©)
```java
ordersStream
    .peek((key, order) -> logger.info("Processing: {}", order))
    .filter(...)
    .peek((key, order) -> logger.info("After filter: {}", order))
```

#### 7. to
ê²°ê³¼ë¥¼ í† í”½ìœ¼ë¡œ ì „ì†¡
```java
statsTable.toStream()
    .to("order-stats", Produced.with(Serdes.String(), statsSerde))
```

### ì´ í”„ë¡œì íŠ¸ì˜ ì‹¤ì œ êµ¬í˜„

#### ì „ì²´ í† í´ë¡œì§€
```java
KStream<String, Order> ordersStream = builder.stream("orders")
    .filter((key, order) -> order != null)           // â‘  Stateless
    .groupBy((key, order) -> "global")               // â‘¡ Stateful (ê·¸ë£¹í™”)
    .aggregate(                                       // â‘¢ Stateful (ì§‘ê³„)
        OrderStats::new,
        (key, order, stats) -> {
            stats.setTotalOrders(stats.getTotalOrders() + 1);
            stats.setTotalSales(stats.getTotalSales() + order.getPrice());

            // ì§€ì—­ë³„ í†µê³„ë„ ì—…ë°ì´íŠ¸
            Map<String, RegionStats> byRegion = stats.getByRegion();
            RegionStats regionStats = byRegion.getOrDefault(
                order.getRegion(),
                new RegionStats(0, 0.0)
            );
            regionStats.setOrders(regionStats.getOrders() + 1);
            regionStats.setSales(regionStats.getSales() + order.getPrice());
            byRegion.put(order.getRegion(), regionStats);

            return stats;
        },
        Materialized.with(Serdes.String(), statsSerde) // RocksDBì— ìƒíƒœ ì €ì¥
    );

statsTable.toStream()
    .to("order-stats", Produced.with(...));           // â‘£ ê²°ê³¼ ë°œí–‰
```

#### ì²˜ë¦¬ íë¦„
```
1. KStream (orders í† í”½)
   â†“ filter (null ì œê±°)
2. Stateless ì²˜ë¦¬
   â†“ groupBy (í‚¤ = "global")
3. Grouped KStream
   â†“ aggregate (ì§‘ê³„)
4. KTable (ìƒíƒœ ì €ì¥)
   â†“ toStream (ìŠ¤íŠ¸ë¦¼ ë³€í™˜)
5. KStream (order-stats í† í”½ìœ¼ë¡œ ë°œí–‰)
```

### Materialized: ìƒíƒœ ì €ì¥ì†Œ

**Materialized**ëŠ” ìƒíƒœë¥¼ ì €ì¥í•˜ëŠ” ë°©ë²•ì„ ì§€ì •í•©ë‹ˆë‹¤.

```java
Materialized.<String, OrderStats, KeyValueStore<Bytes, byte[]>>as("order-stats-store")
    .withKeySerde(Serdes.String())
    .withValueSerde(statsSerde)
    .withCachingEnabled()           // ìºì‹± í™œì„±í™”
    .withLoggingEnabled(...)        // Changelog í™œì„±í™”
```

#### íŠ¹ì§•
- **ì˜êµ¬ ì €ì¥**: RocksDBì— ìƒíƒœ ì €ì¥
- **Fault Tolerance**: Changelog í† í”½ìœ¼ë¡œ ë°±ì—…
- **ë¹ ë¥¸ ì¡°íšŒ**: ë¡œì»¬ ìƒíƒœ ì €ì¥ì†Œì—ì„œ ì¦‰ì‹œ ì¡°íšŒ
- **ì¬ì‹œì‘ ë³µêµ¬**: ì• í”Œë¦¬ì¼€ì´ì…˜ ì¬ì‹œì‘ ì‹œ ìƒíƒœ ë³µì›

#### Changelog í† í”½
Kafka StreamsëŠ” ìë™ìœ¼ë¡œ ìƒíƒœ ë³€ê²½ ì´ë ¥ì„ ì €ì¥í•˜ëŠ” í† í”½ì„ ìƒì„±í•©ë‹ˆë‹¤:
```
order-stats-app-KSTREAM-AGGREGATE-STATE-STORE-0000000004-changelog
```

ì´ í† í”½ì€:
- ìƒíƒœ ì €ì¥ì†Œì˜ ëª¨ë“  ë³€ê²½ì‚¬í•­ ê¸°ë¡
- ì• í”Œë¦¬ì¼€ì´ì…˜ ì¬ì‹œì‘ ì‹œ ìƒíƒœ ë³µêµ¬ì— ì‚¬ìš©
- Compacted topic (ìµœì‹  ê°’ë§Œ ìœ ì§€)

### ì‹¤ìŠµ: ëŒ€ì‹œë³´ë“œì—ì„œ í™•ì¸í•˜ê¸°

http://localhost:8080/dashboard ì—ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ í™•ì¸:

1. **ğŸ² ëœë¤ ì£¼ë¬¸ ìƒì„±** ë²„íŠ¼ í´ë¦­
2. **KStream íŒ¨ë„**: ì£¼ë¬¸ ì´ë²¤íŠ¸ê°€ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ì¶”ê°€ë¨ (INSERT only)
3. **ì²˜ë¦¬ íë¦„ë„**: Producer â†’ KStream â†’ GroupBy â†’ Aggregate â†’ KTable â†’ Output
4. **KTable íŒ¨ë„**: ì§‘ê³„ ìƒíƒœê°€ ì—…ë°ì´íŠ¸ë¨ (UPDATE)
5. **í†µê³„ ì¹´ë“œ**: ì´ ì£¼ë¬¸ìˆ˜, ì´ ë§¤ì¶œ, ì§€ì—­ë³„ í†µê³„ ê°±ì‹ 

---

## ê¸°ìˆ  ìŠ¤íƒ

### Server1 (ì£¼ë¬¸ API & ëŒ€ì‹œë³´ë“œ)
- **ì–¸ì–´**: Node.js 20
- **í”„ë ˆì„ì›Œí¬**: Express.js
- **ë°ì´í„°ë² ì´ìŠ¤**: PostgreSQL 16
- **Kafka í´ë¼ì´ì–¸íŠ¸**: KafkaJS
- **ê¸°íƒ€**: UUID, dotenv, CORS

### Server2 (Kafka Streams)
- **ì–¸ì–´**: Java 17
- **í”„ë ˆì„ì›Œí¬**: Kafka Streams 3.6.1
- **ë¹Œë“œ ë„êµ¬**: Gradle 8.5
- **JSON ì²˜ë¦¬**: Jackson
- **ë¡œê¹…**: SLF4J

### ì¸í”„ë¼
- **ì»¨í…Œì´ë„ˆ**: Docker & Docker Compose
- **Kafka**: Confluent Platform 7.5.0
- **Zookeeper**: Confluent Zookeeper 7.5.0
- **í´ë¼ìš°ë“œ**: AWS (EC2, Security Groups)

---

## ğŸ  ë¡œì»¬ ê°œë°œ í™˜ê²½ì—ì„œ ì‚¬ìš©í•˜ê¸°

> **ğŸ’¡ ë¹ ë¥¸ ì‹œì‘**: ì´ ì„¹ì…˜ì€ ë¡œì»¬ PCì—ì„œ ì „ì²´ ì‹œìŠ¤í…œì„ ì‹¤í–‰í•˜ëŠ” ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì…ë‹ˆë‹¤.

### í•„ìˆ˜ ìš”êµ¬ì‚¬í•­

- **Docker Desktop** ì„¤ì¹˜ ([ë‹¤ìš´ë¡œë“œ](https://www.docker.com/products/docker-desktop))
- **Git** ì„¤ì¹˜
- ìµœì†Œ 8GB RAM, 20GB ì—¬ìœ  ë””ìŠ¤í¬ ê³µê°„

### 3ë¶„ ì•ˆì— ì‹œì‘í•˜ê¸°

#### ğŸ¯ ë°©ë²• 1: ìë™ ì„¤ì • (ê°€ì¥ ê°„ë‹¨!) â­

```bash
# 1. í”„ë¡œì íŠ¸ ë‹¤ìš´ë¡œë“œ
git clone https://github.com/<your-org>/kafka-event-architecture-lab.git
cd kafka-event-architecture-lab

# 2. ìë™ ì„¤ì • ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
./setup-local.sh
```

ì´ ìŠ¤í¬ë¦½íŠ¸ê°€ ìë™ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ì‘ì—…:
- âœ… Docker ì„¤ì¹˜ í™•ì¸
- âœ… ì „ì²´ ì‹œìŠ¤í…œ ì‹œì‘ (Docker Compose)
- âœ… Kafka í† í”½ ìƒì„± (orders, order-stats)
- âœ… Server2 ì¬ì‹œì‘
- âœ… í—¬ìŠ¤ ì²´í¬

ì™„ë£Œë˜ë©´ ë°”ë¡œ `http://localhost:8080/dashboard`ë¡œ ì ‘ì†í•˜ì—¬ ëŒ€ì‹œë³´ë“œë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!

---

#### ğŸ”§ ë°©ë²• 2: ìˆ˜ë™ ì„¤ì • (ë‹¨ê³„ë³„)

#### 1ï¸âƒ£ í”„ë¡œì íŠ¸ ë‹¤ìš´ë¡œë“œ

```bash
# Gitìœ¼ë¡œ í´ë¡ 
git clone https://github.com/<your-org>/kafka-event-architecture-lab.git
cd kafka-event-architecture-lab

# ë˜ëŠ” ZIP ë‹¤ìš´ë¡œë“œ í›„ ì••ì¶• í•´ì œ
```

#### 2ï¸âƒ£ ì‹œìŠ¤í…œ ì‹¤í–‰

```bash
# Docker Composeë¡œ ì „ì²´ ì‹œìŠ¤í…œ ì‹œì‘
cd infra-local
docker-compose up -d --build
```

â±ï¸ **ì´ˆê¸° ë¹Œë“œ ì‹œê°„**: 3-5ë¶„ (ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ + ë¹Œë“œ)
â±ï¸ **ì´í›„ ì‹¤í–‰ ì‹œê°„**: 30ì´ˆ

**ì‹¤í–‰ë˜ëŠ” ì„œë¹„ìŠ¤:**
- âœ… Zookeeper (Kafka ì½”ë””ë„¤ì´í„°)
- âœ… Kafka Broker (ë©”ì‹œì§€ ë¸Œë¡œì»¤)
- âœ… PostgreSQL (ì£¼ë¬¸ ë°ì´í„°ë² ì´ìŠ¤)
- âœ… Server1 (ì£¼ë¬¸ API + ëŒ€ì‹œë³´ë“œ)
- âœ… Server2 (Kafka Streams í†µê³„ ì²˜ë¦¬)

#### 3ï¸âƒ£ Kafka í† í”½ ìƒì„± (ìµœì´ˆ 1íšŒ)

```bash
# orders í† í”½ ìƒì„±
docker exec kafka-broker kafka-topics \
  --bootstrap-server localhost:9092 \
  --create --topic orders \
  --partitions 3 --replication-factor 1 \
  --if-not-exists

# order-stats í† í”½ ìƒì„±
docker exec kafka-broker kafka-topics \
  --bootstrap-server localhost:9092 \
  --create --topic order-stats \
  --partitions 3 --replication-factor 1 \
  --if-not-exists

# Server2 ì¬ì‹œì‘ (í† í”½ ì¸ì‹)
docker restart server2-streams
```

> **ğŸ’¡ TIP**: í† í”½ì€ ìµœì´ˆ 1íšŒë§Œ ìƒì„±í•˜ë©´ ë©ë‹ˆë‹¤. ì´í›„ì—ëŠ” ìë™ìœ¼ë¡œ ìœ ì§€ë©ë‹ˆë‹¤.

#### 4ï¸âƒ£ ë™ì‘ í™•ì¸

```bash
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ ì´ë™
cd ..

# í—¬ìŠ¤ ì²´í¬
./check-health.sh
```

**ì •ìƒ ì¶œë ¥ ì˜ˆì‹œ:**
```
ğŸ¥ Checking service health...

1ï¸âƒ£  Server1 (Order API):
   âœ… Healthy

2ï¸âƒ£  PostgreSQL:
   âœ… Healthy

3ï¸âƒ£  Kafka Broker:
   âœ… Healthy

4ï¸âƒ£  Zookeeper:
   âœ… Healthy

5ï¸âƒ£  Server2 (Kafka Streams):
   âœ… Running
```

#### 5ï¸âƒ£ í…ŒìŠ¤íŠ¸ ì‹¤í–‰

**ë°©ë²• 1: ì›¹ ëŒ€ì‹œë³´ë“œì—ì„œ ì£¼ë¬¸ ìƒì„±** â­ (ê°€ì¥ ì‰¬ì›€!)

ë¸Œë¼ìš°ì €ì—ì„œ `http://localhost:8080/dashboard` ì ‘ì† í›„:
- **ğŸ² ëœë¤ ì£¼ë¬¸ ìƒì„±** ë²„íŠ¼ í´ë¦­ â†’ ì¦‰ì‹œ ëœë¤ ì£¼ë¬¸ ìƒì„±!
- ë˜ëŠ” í¼ì— ì§ì ‘ ì…ë ¥í•˜ì—¬ **ì£¼ë¬¸ ìƒì„±** ë²„íŠ¼ í´ë¦­

ì‹¤ì‹œê°„ìœ¼ë¡œ ë‹¤ìŒì„ í™•ì¸ ê°€ëŠ¥:
- ì²˜ë¦¬ íë¦„ë„ ì• ë‹ˆë©”ì´ì…˜ (Producer â†’ KStream â†’ ... â†’ Output)
- KStream íŒ¨ë„ì— ìƒˆ ì´ë²¤íŠ¸ í‘œì‹œ
- KTable íŒ¨ë„ì— ì§‘ê³„ ìƒíƒœ ì—…ë°ì´íŠ¸
- í†µê³„ ì¦‰ì‹œ ê°±ì‹ 

**ë°©ë²• 2: ì‰˜ ìŠ¤í¬ë¦½íŠ¸ë¡œ ëŒ€ëŸ‰ ìƒì„±**

```bash
# 10ê°œì˜ ìƒ˜í”Œ ì£¼ë¬¸ ìƒì„±
./test-orders.sh 10
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```
ğŸ“¦ Creating 10 sample orders...
âœ… Order 1 created: abc123... | Seoul | â‚©25,000
âœ… Order 2 created: def456... | Busan | â‚©18,000
...
âœ… Created 10 orders
ğŸ“Š Check dashboard at: http://localhost:8080/dashboard
```

**ë°©ë²• 3: API ì§ì ‘ í˜¸ì¶œ**

ì•„ë˜ "API ì§ì ‘ í…ŒìŠ¤íŠ¸" ì„¹ì…˜ ì°¸ì¡°

#### 6ï¸âƒ£ ëŒ€ì‹œë³´ë“œ í™•ì¸

ë¸Œë¼ìš°ì €ë¥¼ ì—´ê³  ë‹¤ìŒ URLì— ì ‘ì†:

```
http://localhost:8080/dashboard
```

**ëŒ€ì‹œë³´ë“œì—ì„œ í™•ì¸ ê°€ëŠ¥í•œ í•­ëª©:**
- ğŸŒŠ **Kafka Streams ì²˜ë¦¬ íë¦„ë„**: Producer â†’ KStream â†’ GroupBy â†’ Aggregate â†’ KTable â†’ Output
- ğŸ“¥ **KStream ì‹¤ì‹œê°„ ì´ë²¤íŠ¸**: ì£¼ë¬¸ ìƒì„± ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ (INSERT only)
- ğŸ“Š **KTable ì§‘ê³„ ìƒíƒœ**: ìƒíƒœ ë³€ê²½ ì´ë ¥ ì¶”ì  (UPDATE ê°€ëŠ¥)
- ğŸ² **ëœë¤ ì£¼ë¬¸ ìƒì„± ë²„íŠ¼**: í´ë¦­ í•œ ë²ˆìœ¼ë¡œ ì¦‰ì‹œ ëœë¤ ì£¼ë¬¸ ìƒì„±
- ğŸ“ **ìˆ˜ë™ ì£¼ë¬¸ ì…ë ¥ í¼**: ê³ ê°ID, ìƒì ID, ì§€ì—­, ê°€ê²© ì§ì ‘ ì…ë ¥
- ğŸ“¦ **ì´ ì£¼ë¬¸ ìˆ˜**: KTableì—ì„œ ì§‘ê³„ëœ ì´ ì£¼ë¬¸ ê±´ìˆ˜
- ğŸ’° **ì´ ë§¤ì¶œ**: KTableì—ì„œ ì§‘ê³„ëœ ì´ ë§¤ì¶œì•¡
- ğŸ“ **ì§€ì—­ë³„ í†µê³„**: ì„œìš¸, ë¶€ì‚°, ì¸ì²œ, ëŒ€êµ¬, ê´‘ì£¼ ì§€ì—­ë³„ ì£¼ë¬¸ ìˆ˜ ë° ë§¤ì¶œ
- ğŸ”„ **ìë™ ê°±ì‹ **: 3ì´ˆë§ˆë‹¤ ìë™ ì—…ë°ì´íŠ¸

### ì¼ìƒì ì¸ ì‚¬ìš© ë°©ë²•

#### ì‹œìŠ¤í…œ ì‹œì‘

```bash
cd infra-local
docker-compose up -d
```

#### ì‹œìŠ¤í…œ ì¢…ë£Œ

```bash
cd infra-local
docker-compose down
```

#### ì™„ì „ ì´ˆê¸°í™” (ë°ì´í„° í¬í•¨)

```bash
cd infra-local
docker-compose down -v
```

#### ë¡œê·¸ í™•ì¸

```bash
# ì „ì²´ ë¡œê·¸ (ì‹¤ì‹œê°„)
docker-compose logs -f

# Server1 ë¡œê·¸ë§Œ
docker logs server1-app -f

# Server2 ë¡œê·¸ë§Œ
docker logs server2-streams -f

# Kafka ë¡œê·¸ë§Œ
docker logs kafka-broker -f
```

#### íŠ¹ì • ì„œë¹„ìŠ¤ë§Œ ì¬ì‹œì‘

```bash
docker restart server1-app      # Server1ë§Œ ì¬ì‹œì‘
docker restart server2-streams  # Server2ë§Œ ì¬ì‹œì‘
docker restart kafka-broker     # Kafkaë§Œ ì¬ì‹œì‘
```

### API ì§ì ‘ í…ŒìŠ¤íŠ¸

#### ì£¼ë¬¸ ìƒì„±

```bash
curl -X POST http://localhost:8080/orders \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": "user-001",
    "store_id": "store-777",
    "region": "Seoul",
    "price": 25000
  }'
```

**ì‘ë‹µ:**
```json
{
  "success": true,
  "order": {
    "order_id": "uuid-...",
    "user_id": "user-001",
    "store_id": "store-777",
    "region": "Seoul",
    "price": 25000,
    "status": "CREATED",
    "created_at": "2025-11-17T14:30:00.123Z"
  }
}
```

#### í†µê³„ ì¡°íšŒ

```bash
# ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©
./check-stats.sh

# ë˜ëŠ” curl ì§ì ‘ ì‚¬ìš©
curl http://localhost:8080/stats | jq
```

**ì‘ë‹µ:**
```json
{
  "success": true,
  "stats": {
    "totalOrders": 100,
    "totalSales": 2500000,
    "byRegion": {
      "Seoul": {"orders": 50, "sales": 1250000},
      "Busan": {"orders": 30, "sales": 750000},
      "Incheon": {"orders": 20, "sales": 500000}
    },
    "lastUpdated": "2025-11-17T14:30:00.123Z"
  }
}
```

#### ì£¼ë¬¸ ëª©ë¡ ì¡°íšŒ

```bash
curl http://localhost:8080/orders?limit=10
```

### ë¬¸ì œ í•´ê²°

#### í¬íŠ¸ê°€ ì´ë¯¸ ì‚¬ìš© ì¤‘

**ì¦ìƒ:**
```
Error: Ports are not available: port is already allocated
```

**í•´ê²°:**
```bash
# í¬íŠ¸ ì‚¬ìš© ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ í™•ì¸
lsof -i :8080  # Server1
lsof -i :9092  # Kafka
lsof -i :5432  # PostgreSQL

# í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ë˜ëŠ” Docker Composeì—ì„œ í¬íŠ¸ ë³€ê²½
```

#### Server2ê°€ ê³„ì† ì¬ì‹œì‘ë¨

**ì›ì¸**: Kafka í† í”½ì´ ìƒì„±ë˜ì§€ ì•ŠìŒ

**í•´ê²°:**
```bash
# í† í”½ í™•ì¸
docker exec kafka-broker kafka-topics --bootstrap-server localhost:9092 --list

# í† í”½ì´ ì—†ìœ¼ë©´ 3ë‹¨ê³„ë¡œ ëŒì•„ê°€ì„œ í† í”½ ìƒì„±
```

#### í†µê³„ê°€ 0ìœ¼ë¡œ í‘œì‹œë¨

**í•´ê²°:**
```bash
# 1. Server2 ë¡œê·¸ í™•ì¸
docker logs server2-streams --tail 50

# 2. Server2 ì¬ì‹œì‘
docker restart server2-streams

# 3. ìƒˆ ì£¼ë¬¸ ìƒì„±
./test-orders.sh 5

# 4. í†µê³„ ë‹¤ì‹œ í™•ì¸
./check-stats.sh
```

#### Docker ë©”ëª¨ë¦¬ ë¶€ì¡±

**ì¦ìƒ:**
```
Error: docker: Error response from daemon: OOM command not allowed
```

**í•´ê²°:**
1. Docker Desktop ì„¤ì • ì—´ê¸°
2. Resources â†’ Memory ì„¤ì •ì„ 8GB ì´ìƒìœ¼ë¡œ ì¦ê°€
3. Apply & Restart

### ê°œë°œ íŒ

#### ì½”ë“œ ìˆ˜ì • í›„ ì¬ë°°í¬

**Server1 (Node.js) ìˆ˜ì • ì‹œ:**
```bash
cd infra-local
docker-compose stop server1
docker-compose build server1
docker-compose up -d server1
```

**Server2 (Java) ìˆ˜ì • ì‹œ:**
```bash
cd infra-local
docker-compose stop server2
docker-compose build server2
docker-compose up -d server2
```

#### ì‹¤ì‹œê°„ ë¡œê·¸ ëª¨ë‹ˆí„°ë§

```bash
# í„°ë¯¸ë„ì„ 4ê°œë¡œ ë¶„í• í•˜ì—¬ ê°ê° ëª¨ë‹ˆí„°ë§
# í„°ë¯¸ë„ 1
docker logs server1-app -f

# í„°ë¯¸ë„ 2
docker logs server2-streams -f

# í„°ë¯¸ë„ 3
docker logs kafka-broker -f

# í„°ë¯¸ë„ 4
./test-orders.sh 1  # ë°˜ë³µ ì‹¤í–‰
```

#### Kafka ë©”ì‹œì§€ ì§ì ‘ í™•ì¸

```bash
# orders í† í”½ ë©”ì‹œì§€ í™•ì¸
docker exec -it kafka-broker kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic orders \
  --from-beginning \
  --max-messages 10

# order-stats í† í”½ ë©”ì‹œì§€ í™•ì¸
docker exec -it kafka-broker kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic order-stats \
  --from-beginning \
  --max-messages 10
```

### ì£¼ìš” ì—”ë“œí¬ì¸íŠ¸

| ì„œë¹„ìŠ¤ | URL | ì„¤ëª… |
|--------|-----|------|
| ëŒ€ì‹œë³´ë“œ | http://localhost:8080/dashboard | ì‹¤ì‹œê°„ í†µê³„ ëŒ€ì‹œë³´ë“œ |
| í—¬ìŠ¤ì²´í¬ | http://localhost:8080/health | API ìƒíƒœ í™•ì¸ |
| ì£¼ë¬¸ ìƒì„± | POST http://localhost:8080/orders | ìƒˆ ì£¼ë¬¸ ìƒì„± |
| ì£¼ë¬¸ ì¡°íšŒ | GET http://localhost:8080/orders | ì£¼ë¬¸ ëª©ë¡ |
| í†µê³„ ì¡°íšŒ | GET http://localhost:8080/stats | ì‹¤ì‹œê°„ í†µê³„ |

### ì„±ëŠ¥ íŠœë‹ (ì„ íƒì‚¬í•­)

#### Kafka íŒŒí‹°ì…˜ ì¦ê°€

```bash
# ì²˜ë¦¬ëŸ‰ ì¦ê°€ë¥¼ ìœ„í•´ íŒŒí‹°ì…˜ ìˆ˜ ì¦ê°€
docker exec kafka-broker kafka-topics \
  --bootstrap-server localhost:9092 \
  --alter --topic orders \
  --partitions 6
```

#### ëŒ€ì‹œë³´ë“œ ê°±ì‹  ì£¼ê¸° ë³€ê²½

`server1-app/public/dashboard.html` íŒŒì¼ ìˆ˜ì •:
```javascript
// 5ì´ˆ â†’ 2ì´ˆë¡œ ë³€ê²½
const REFRESH_INTERVAL = 2000;
```

### ë‹¤ìŒ ë‹¨ê³„

- ğŸ“– **ìƒì„¸ ê°€ì´ë“œ**: [ë¡œì»¬ í™˜ê²½ ì‹¤ìŠµ (ìƒì„¸)](#ë¡œì»¬-í™˜ê²½-ì‹¤ìŠµ) - ë‹¨ê³„ë³„ ìì„¸í•œ ì„¤ëª…
- â˜ï¸ **AWS ë°°í¬**: [AWS ë°°í¬ ê°€ì´ë“œ](#aws-ë°°í¬-ê°€ì´ë“œ) - ì‹¤ì œ ì„œë²„ì— ë°°í¬
- ğŸ”§ **ì½”ë“œ ì´í•´**: `server1-app/README.md`, `server2-streams/README.md` ì°¸ê³ 

---

## ë¡œì»¬ í™˜ê²½ ì‹¤ìŠµ (ìƒì„¸)

> **ğŸ’¡ ì°¸ê³ **: ë¹ ë¥´ê²Œ ì‹œì‘í•˜ë ¤ë©´ [ë¡œì»¬ ê°œë°œ í™˜ê²½ì—ì„œ ì‚¬ìš©í•˜ê¸°](#-ë¡œì»¬-ê°œë°œ-í™˜ê²½ì—ì„œ-ì‚¬ìš©í•˜ê¸°)ë¥¼ ë¨¼ì € ë³´ì„¸ìš”.

ë¡œì»¬ í™˜ê²½ì—ì„œ ì „ì²´ ì‹œìŠ¤í…œì„ Docker Composeë¡œ ì‹¤í–‰í•˜ê³  í…ŒìŠ¤íŠ¸í•˜ëŠ” ìƒì„¸ ê°€ì´ë“œì…ë‹ˆë‹¤.

### ì‚¬ì „ ìš”êµ¬ì‚¬í•­

ë‹¤ìŒ ì†Œí”„íŠ¸ì›¨ì–´ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤:

```bash
# Docker ë²„ì „ í™•ì¸ (20.10 ì´ìƒ)
docker --version

# Docker Compose ë²„ì „ í™•ì¸ (2.0 ì´ìƒ)
docker-compose --version

# (ì„ íƒ) curl - API í…ŒìŠ¤íŠ¸ìš©
curl --version

# (ì„ íƒ) jq - JSON íŒŒì‹±ìš©
jq --version
```

### Step 1: í”„ë¡œì íŠ¸ í´ë¡ 

```bash
# ë¦¬í¬ì§€í† ë¦¬ í´ë¡ 
git clone https://github.com/<your-org>/kafka-event-architecture-lab.git
cd kafka-event-architecture-lab

# ë””ë ‰í† ë¦¬ êµ¬ì¡° í™•ì¸
ls -la
```

**ì˜ˆìƒ ì¶œë ¥:**
```
drwxr-xr-x  server1-app/         # ì£¼ë¬¸ API & ëŒ€ì‹œë³´ë“œ
drwxr-xr-x  server2-streams/     # Kafka Streams ì²˜ë¦¬
drwxr-xr-x  infra-local/         # Docker Compose ì„¤ì •
-rwxr-xr-x  test-orders.sh       # ì£¼ë¬¸ ìƒì„± í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
-rwxr-xr-x  check-stats.sh       # í†µê³„ í™•ì¸ ìŠ¤í¬ë¦½íŠ¸
-rwxr-xr-x  check-health.sh      # í—¬ìŠ¤ ì²´í¬ ìŠ¤í¬ë¦½íŠ¸
-rw-r--r--  README.md
```

### Step 2: ì „ì²´ ì‹œìŠ¤í…œ ì‹¤í–‰

Docker Composeë¡œ ëª¨ë“  ì„œë¹„ìŠ¤ë¥¼ í•œ ë²ˆì— ì‹œì‘í•©ë‹ˆë‹¤.

```bash
cd infra-local

# ëª¨ë“  ì„œë¹„ìŠ¤ ë¹Œë“œ ë° ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ)
docker-compose up -d --build
```

**ì‹¤í–‰ ê³¼ì •:**
1. Zookeeper ì»¨í…Œì´ë„ˆ ì‹œì‘
2. Kafka ë¸Œë¡œì»¤ ì»¨í…Œì´ë„ˆ ì‹œì‘
3. PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì‹œì‘
4. Server1 ì´ë¯¸ì§€ ë¹Œë“œ ë° ì‹œì‘
5. Server2 ì´ë¯¸ì§€ ë¹Œë“œ ë° ì‹œì‘

**ì˜ˆìƒ ì†Œìš” ì‹œê°„:** ì´ˆê¸° ë¹Œë“œ 3-5ë¶„, ì´í›„ ì‹¤í–‰ 30ì´ˆ~1ë¶„

```bash
# ì‹¤í–‰ ì¤‘ì¸ ì»¨í…Œì´ë„ˆ í™•ì¸
docker-compose ps
```

**ì˜ˆìƒ ì¶œë ¥:**
```
NAME               IMAGE                    STATUS        PORTS
kafka-broker       confluentinc/cp-kafka    Up (healthy)  0.0.0.0:9092->9092/tcp
postgres-orders    postgres:16-alpine       Up (healthy)  0.0.0.0:5432->5432/tcp
server1-app        infra-local-server1      Up            0.0.0.0:8080->8080/tcp
server2-streams    infra-local-server2      Up
zookeeper          confluentinc/cp-zookeeper Up (healthy) 0.0.0.0:2181->2181/tcp
```

### Step 3: ì„œë¹„ìŠ¤ í—¬ìŠ¤ ì²´í¬

ëª¨ë“  ì„œë¹„ìŠ¤ê°€ ì •ìƒì ìœ¼ë¡œ ì‹¤í–‰ë˜ê³  ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.

```bash
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ ì´ë™
cd ..

# í—¬ìŠ¤ ì²´í¬ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
./check-health.sh
```

**ì˜ˆìƒ ì¶œë ¥:**
```
ğŸ¥ Checking service health...

1ï¸âƒ£  Server1 (Order API):
   âœ… Healthy

2ï¸âƒ£  PostgreSQL:
   âœ… Healthy

3ï¸âƒ£  Kafka Broker:
   âœ… Healthy

4ï¸âƒ£  Zookeeper:
   âœ… Healthy

5ï¸âƒ£  Server2 (Kafka Streams):
   âœ… Running

ğŸ“‹ Summary:
NAMES              STATUS          PORTS
kafka-broker       Up 2 minutes    0.0.0.0:9092->9092/tcp
postgres-orders    Up 2 minutes    0.0.0.0:5432->5432/tcp
server1-app        Up 1 minute     0.0.0.0:8080->8080/tcp
server2-streams    Up 1 minute
zookeeper          Up 2 minutes    0.0.0.0:2181->2181/tcp
```

### Step 4: ë¡œê·¸ í™•ì¸

ê° ì„œë¹„ìŠ¤ì˜ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì—¬ ì •ìƒ ì‘ë™ ì—¬ë¶€ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤.

```bash
# Server1 ë¡œê·¸ í™•ì¸
docker logs server1-app --tail 50

# Server2 ë¡œê·¸ í™•ì¸
docker logs server2-streams --tail 50

# Kafka ë¡œê·¸ í™•ì¸
docker logs kafka-broker --tail 50

# ì „ì²´ ë¡œê·¸ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
cd infra-local
docker-compose logs -f
```

**Server1 ì •ìƒ ë¡œê·¸ ì˜ˆì‹œ:**
```
âœ… Database schema initialized
âœ… Kafka Producer connected
âœ… Kafka Consumer connected and subscribed to order-stats
ğŸš€ Server1 running on http://localhost:8080
ğŸ“Š Dashboard: http://localhost:8080/dashboard
```

**Server2 ì •ìƒ ë¡œê·¸ ì˜ˆì‹œ:**
```
Starting Kafka Streams application...
Input topic: orders
Output topic: order-stats
âœ… Topology built successfully
âœ… Kafka Streams application started successfully
```

### Step 5: ëŒ€ì‹œë³´ë“œ í™•ì¸

ì›¹ ë¸Œë¼ìš°ì €ë¥¼ ì—´ê³  ëŒ€ì‹œë³´ë“œì— ì ‘ì†í•©ë‹ˆë‹¤.

```bash
# ë¸Œë¼ìš°ì €ì—ì„œ ì—´ê¸°
open http://localhost:8080/dashboard
# ë˜ëŠ”
# Windows: start http://localhost:8080/dashboard
# Linux: xdg-open http://localhost:8080/dashboard
```

**ëŒ€ì‹œë³´ë“œ í™”ë©´:**
- ì´ ì£¼ë¬¸ ìˆ˜
- ì´ ë§¤ì¶œ
- ì§€ì—­ë³„ í†µê³„
- ìµœê·¼ ì£¼ë¬¸ ë‚´ì—­

### Step 6: ì£¼ë¬¸ ìƒì„± í…ŒìŠ¤íŠ¸

#### ë°©ë²• 1: í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš© (ê¶Œì¥)

```bash
# 10ê°œì˜ ìƒ˜í”Œ ì£¼ë¬¸ ìƒì„±
./test-orders.sh 10
```

**ì˜ˆìƒ ì¶œë ¥:**
```
ğŸ“¦ Creating 10 sample orders...
ğŸ”— Server URL: http://localhost:8080

âœ… Order 1 created: a1b2c3d4-... | Seoul | â‚©25000
âœ… Order 2 created: e5f6g7h8-... | Busan | â‚©18000
âœ… Order 3 created: i9j0k1l2-... | Incheon | â‚©32000
...

âœ… Created 10 orders
ğŸ“Š Check dashboard at: http://localhost:8080/dashboard
```

#### ë°©ë²• 2: curlë¡œ ìˆ˜ë™ ìƒì„±

```bash
# ë‹¨ì¼ ì£¼ë¬¸ ìƒì„±
curl -X POST http://localhost:8080/orders \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": "user-001",
    "store_id": "store-777",
    "region": "Seoul",
    "price": 25000
  }'
```

**ì„±ê³µ ì‘ë‹µ:**
```json
{
  "success": true,
  "order": {
    "order_id": "a1b2c3d4-e5f6-g7h8-i9j0-k1l2m3n4o5p6",
    "user_id": "user-001",
    "store_id": "store-777",
    "region": "Seoul",
    "price": 25000,
    "status": "CREATED",
    "created_at": "2025-11-17T14:30:00.123Z"
  }
}
```

#### ë°©ë²• 3: ëŒ€ì‹œë³´ë“œ UIì—ì„œ ìƒì„±

1. http://localhost:8080/dashboard ì ‘ì†
2. "ìƒˆ ì£¼ë¬¸ ìƒì„±" í¼ ì‘ì„±
3. "ì£¼ë¬¸ ìƒì„±" ë²„íŠ¼ í´ë¦­
4. ì‹¤ì‹œê°„ìœ¼ë¡œ í†µê³„ê°€ ê°±ì‹ ë˜ëŠ” ê²ƒ í™•ì¸

### Step 7: í†µê³„ í™•ì¸

```bash
# í˜„ì¬ í†µê³„ ì¡°íšŒ
./check-stats.sh
```

**ì˜ˆìƒ ì¶œë ¥:**
```
ğŸ“Š Fetching current statistics from http://localhost:8080/stats...

Response:
{
  "success": true,
  "stats": {
    "totalOrders": 10,
    "totalSales": 245000,
    "byRegion": {
      "Seoul": {
        "orders": 4,
        "sales": 98000
      },
      "Busan": {
        "orders": 3,
        "sales": 72000
      },
      "Incheon": {
        "orders": 3,
        "sales": 75000
      }
    },
    "lastUpdated": "2025-11-17T14:30:15.456Z"
  }
}

ğŸ“Š Dashboard URL: http://localhost:8080/dashboard
```

### Step 8: Kafka í† í”½ í™•ì¸ (ê³ ê¸‰)

Kafka ë‚´ë¶€ì˜ ë©”ì‹œì§€ë¥¼ ì§ì ‘ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```bash
# Kafka ì»¨í…Œì´ë„ˆ ë‚´ë¶€ë¡œ ì§„ì…
docker exec -it kafka-broker bash

# orders í† í”½ ë©”ì‹œì§€ í™•ì¸
kafka-console-consumer --bootstrap-server localhost:9092 \
  --topic orders \
  --from-beginning \
  --max-messages 5

# order-stats í† í”½ ë©”ì‹œì§€ í™•ì¸
kafka-console-consumer --bootstrap-server localhost:9092 \
  --topic order-stats \
  --from-beginning \
  --max-messages 5

# í† í”½ ë¦¬ìŠ¤íŠ¸ í™•ì¸
kafka-topics --bootstrap-server localhost:9092 --list

# í† í”½ ìƒì„¸ ì •ë³´ í™•ì¸
kafka-topics --bootstrap-server localhost:9092 \
  --describe \
  --topic orders

# ì»¨í…Œì´ë„ˆì—ì„œ ë‚˜ê°€ê¸°
exit
```

### Step 9: ë¶€í•˜ í…ŒìŠ¤íŠ¸

ëŒ€ëŸ‰ì˜ ì£¼ë¬¸ì„ ìƒì„±í•˜ì—¬ ì‹œìŠ¤í…œì˜ ì‹¤ì‹œê°„ ì²˜ë¦¬ ëŠ¥ë ¥ì„ í™•ì¸í•©ë‹ˆë‹¤.

```bash
# 100ê°œì˜ ì£¼ë¬¸ ìƒì„±
./test-orders.sh 100

# ëŒ€ì‹œë³´ë“œì—ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ í†µê³„ê°€ ê°±ì‹ ë˜ëŠ”ì§€ í™•ì¸
open http://localhost:8080/dashboard
```

### Step 10: ì‹œìŠ¤í…œ ì¢…ë£Œ

```bash
cd infra-local

# ëª¨ë“  ì»¨í…Œì´ë„ˆ ì •ì§€ ë° ì‚­ì œ
docker-compose down

# ë³¼ë¥¨ê¹Œì§€ ì‚­ì œ (ë°ì´í„° ì™„ì „ ì´ˆê¸°í™”)
docker-compose down -v

# ì´ë¯¸ì§€ê¹Œì§€ ì‚­ì œ
docker-compose down -v --rmi all
```

---

## AWS ë°°í¬ ê°€ì´ë“œ

ì‹¤ì œ 2ëŒ€ì˜ AWS EC2 ì¸ìŠ¤í„´ìŠ¤ì— ë°°í¬í•˜ëŠ” ì „ì²´ ê³¼ì •ì…ë‹ˆë‹¤.

### ì•„í‚¤í…ì²˜ êµ¬ì„±

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              AWS VPC                         â”‚
â”‚                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  EC2 Instance 1 (Server1)              â”‚ â”‚
â”‚  â”‚  Ubuntu 22.04 LTS                      â”‚ â”‚
â”‚  â”‚  â€¢ PostgreSQL Container                â”‚ â”‚
â”‚  â”‚  â€¢ Server1 Application Container       â”‚ â”‚
â”‚  â”‚  Public IP: <SERVER1_IP>               â”‚ â”‚
â”‚  â”‚  Port: 8080 (HTTP)                     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                     â”‚                        â”‚
â”‚                     â”‚ Kafka                  â”‚
â”‚                     â–¼ (9092)                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  EC2 Instance 2 (Server2)              â”‚ â”‚
â”‚  â”‚  Ubuntu 22.04 LTS                      â”‚ â”‚
â”‚  â”‚  â€¢ Zookeeper Container                 â”‚ â”‚
â”‚  â”‚  â€¢ Kafka Broker Container              â”‚ â”‚
â”‚  â”‚  â€¢ Server2 Streams Container           â”‚ â”‚
â”‚  â”‚  Public IP: <SERVER2_IP>               â”‚ â”‚
â”‚  â”‚  Port: 9092 (Kafka)                    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ì‚¬ì „ ì¤€ë¹„

#### 1. AWS EC2 ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

**Server1ìš© EC2:**
- AMI: Ubuntu 22.04 LTS
- ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…: t3.medium (2 vCPU, 4GB RAM) ì´ìƒ
- ìŠ¤í† ë¦¬ì§€: 30GB GP3
- ë³´ì•ˆ ê·¸ë£¹:
  - SSH (22): ë‚´ IPì—ì„œë§Œ
  - HTTP (8080): 0.0.0.0/0

**Server2ìš© EC2:**
- AMI: Ubuntu 22.04 LTS
- ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…: t3.medium (2 vCPU, 4GB RAM) ì´ìƒ
- ìŠ¤í† ë¦¬ì§€: 30GB GP3
- ë³´ì•ˆ ê·¸ë£¹:
  - SSH (22): ë‚´ IPì—ì„œë§Œ
  - Kafka (9092): Server1 ë³´ì•ˆ ê·¸ë£¹

```bash
# AWS CLIë¡œ ë³´ì•ˆ ê·¸ë£¹ ìƒì„± ì˜ˆì‹œ

# Server1 ë³´ì•ˆ ê·¸ë£¹
aws ec2 create-security-group \
  --group-name kafka-lab-server1-sg \
  --description "Security group for Server1"

aws ec2 authorize-security-group-ingress \
  --group-id <SERVER1_SG_ID> \
  --protocol tcp \
  --port 22 \
  --cidr <YOUR_IP>/32

aws ec2 authorize-security-group-ingress \
  --group-id <SERVER1_SG_ID> \
  --protocol tcp \
  --port 8080 \
  --cidr 0.0.0.0/0

# Server2 ë³´ì•ˆ ê·¸ë£¹
aws ec2 create-security-group \
  --group-name kafka-lab-server2-sg \
  --description "Security group for Server2"

aws ec2 authorize-security-group-ingress \
  --group-id <SERVER2_SG_ID> \
  --protocol tcp \
  --port 22 \
  --cidr <YOUR_IP>/32

aws ec2 authorize-security-group-ingress \
  --group-id <SERVER2_SG_ID> \
  --protocol tcp \
  --port 9092 \
  --source-group <SERVER1_SG_ID>
```

#### 2. í‚¤í˜ì–´ ì„¤ì •

```bash
# í‚¤í˜ì–´ ê¶Œí•œ ì„¤ì •
chmod 400 ~/your-keypair.pem
```

### Server2 ë°°í¬ (Kafka ë¸Œë¡œì»¤ + Streams)

Server2ë¥¼ ë¨¼ì € ë°°í¬í•˜ì—¬ Kafka ì¸í”„ë¼ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.

#### Step 1: Server2 ì ‘ì†

```bash
# Server2 EC2ì— SSH ì ‘ì†
ssh -i ~/your-keypair.pem ubuntu@<SERVER2_PUBLIC_IP>
```

#### Step 2: Docker ì„¤ì¹˜

```bash
# ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì—…ë°ì´íŠ¸
sudo apt-get update
sudo apt-get upgrade -y

# Docker ì„¤ì¹˜
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh

# Docker Compose ì„¤ì¹˜
sudo curl -L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" \
  -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose

# í˜„ì¬ ì‚¬ìš©ìë¥¼ docker ê·¸ë£¹ì— ì¶”ê°€
sudo usermod -aG docker $USER

# ë¡œê·¸ì•„ì›ƒ í›„ ì¬ì ‘ì†í•˜ì—¬ ê¶Œí•œ ì ìš©
exit
ssh -i ~/your-keypair.pem ubuntu@<SERVER2_PUBLIC_IP>

# Docker ì„¤ì¹˜ í™•ì¸
docker --version
docker-compose --version
```

#### Step 3: í”„ë¡œì íŠ¸ í´ë¡  ë° í™˜ê²½ ì„¤ì •

```bash
# Git ì„¤ì¹˜
sudo apt-get install -y git

# í”„ë¡œì íŠ¸ í´ë¡ 
cd ~
git clone https://github.com/<your-org>/kafka-event-architecture-lab.git
cd kafka-event-architecture-lab/server2-streams
```

#### Step 4: Kafka ë„¤íŠ¸ì›Œí¬ ìƒì„±

```bash
# Docker ë„¤íŠ¸ì›Œí¬ ìƒì„±
docker network create kafka-network
```

#### Step 5: Zookeeper ì‹¤í–‰

```bash
# Zookeeper ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker run -d \
  --name zookeeper \
  --network kafka-network \
  -e ZOOKEEPER_CLIENT_PORT=2181 \
  -e ZOOKEEPER_TICK_TIME=2000 \
  -p 2181:2181 \
  confluentinc/cp-zookeeper:7.5.0

# Zookeeper ë¡œê·¸ í™•ì¸
docker logs zookeeper --tail 20

# Zookeeper ìƒíƒœ í™•ì¸
docker exec zookeeper zkServer.sh status
```

#### Step 6: Kafka ë¸Œë¡œì»¤ ì‹¤í–‰

```bash
# í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ì˜ Private IP í™•ì¸
PRIVATE_IP=$(curl -s http://169.254.169.254/latest/meta-data/local-ipv4)
PUBLIC_IP=$(curl -s http://169.254.169.254/latest/meta-data/public-ipv4)

echo "Private IP: $PRIVATE_IP"
echo "Public IP: $PUBLIC_IP"

# Kafka ë¸Œë¡œì»¤ ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker run -d \
  --name kafka-broker \
  --network kafka-network \
  -e KAFKA_BROKER_ID=1 \
  -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
  -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://$PRIVATE_IP:9092,PLAINTEXT_HOST://$PUBLIC_IP:9092 \
  -e KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT \
  -e KAFKA_INTER_BROKER_LISTENER_NAME=PLAINTEXT \
  -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
  -e KAFKA_AUTO_CREATE_TOPICS_ENABLE=true \
  -e KAFKA_LOG_RETENTION_HOURS=168 \
  -p 9092:9092 \
  confluentinc/cp-kafka:7.5.0

# Kafka ë¡œê·¸ í™•ì¸ (ì •ìƒ ì‹œì‘ê¹Œì§€ ì•½ 30ì´ˆ ì†Œìš”)
docker logs kafka-broker -f
# Ctrl+Cë¡œ ë¡œê·¸ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ

# Kafka ì •ìƒ ì‘ë™ í™•ì¸
docker exec kafka-broker kafka-broker-api-versions \
  --bootstrap-server localhost:9092
```

#### Step 7: Kafka í† í”½ ìƒì„±

```bash
# orders í† í”½ ìƒì„±
docker exec kafka-broker kafka-topics \
  --bootstrap-server localhost:9092 \
  --create \
  --topic orders \
  --partitions 3 \
  --replication-factor 1

# order-stats í† í”½ ìƒì„±
docker exec kafka-broker kafka-topics \
  --bootstrap-server localhost:9092 \
  --create \
  --topic order-stats \
  --partitions 3 \
  --replication-factor 1

# í† í”½ ìƒì„± í™•ì¸
docker exec kafka-broker kafka-topics \
  --bootstrap-server localhost:9092 \
  --list

# í† í”½ ìƒì„¸ ì •ë³´ í™•ì¸
docker exec kafka-broker kafka-topics \
  --bootstrap-server localhost:9092 \
  --describe \
  --topic orders
```

#### Step 8: Server2 Streams ì• í”Œë¦¬ì¼€ì´ì…˜ ë¹Œë“œ ë° ì‹¤í–‰

```bash
cd ~/kafka-event-architecture-lab/server2-streams

# Docker ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t server2-streams:latest .

# ë¹Œë“œ í™•ì¸
docker images | grep server2-streams

# Server2 ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰
docker run -d \
  --name server2-streams \
  --network kafka-network \
  -e KAFKA_BOOTSTRAP_SERVERS=kafka-broker:9092 \
  -e APPLICATION_ID=order-stats-app \
  --restart unless-stopped \
  server2-streams:latest

# ë¡œê·¸ í™•ì¸
docker logs server2-streams -f
# Ctrl+Cë¡œ ì¢…ë£Œ
```

**ì •ìƒ ì‹¤í–‰ ë¡œê·¸ ì˜ˆì‹œ:**
```
Starting Kafka Streams application...
Input topic: orders
Output topic: order-stats
âœ… Topology built successfully
âœ… Kafka Streams application started successfully
```

#### Step 9: Server2 ìƒíƒœ í™•ì¸

```bash
# ì‹¤í–‰ ì¤‘ì¸ ì»¨í…Œì´ë„ˆ í™•ì¸
docker ps

# ì „ì²´ ìƒíƒœ í™•ì¸
docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"
```

**ì˜ˆìƒ ì¶œë ¥:**
```
NAMES            STATUS              PORTS
server2-streams  Up 1 minute
kafka-broker     Up 3 minutes        0.0.0.0:9092->9092/tcp
zookeeper        Up 4 minutes        0.0.0.0:2181->2181/tcp
```

### Server1 ë°°í¬ (ì£¼ë¬¸ API + ëŒ€ì‹œë³´ë“œ)

#### Step 1: Server1 ì ‘ì†

ë¡œì»¬ í„°ë¯¸ë„ì—ì„œ ìƒˆ ì„¸ì…˜ì„ ì—´ì–´ Server1ì— ì ‘ì†í•©ë‹ˆë‹¤.

```bash
# Server1 EC2ì— SSH ì ‘ì†
ssh -i ~/your-keypair.pem ubuntu@<SERVER1_PUBLIC_IP>
```

#### Step 2: Docker ì„¤ì¹˜

```bash
# ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì—…ë°ì´íŠ¸
sudo apt-get update
sudo apt-get upgrade -y

# Docker ì„¤ì¹˜
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh

# Docker Compose ì„¤ì¹˜
sudo curl -L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" \
  -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose

# í˜„ì¬ ì‚¬ìš©ìë¥¼ docker ê·¸ë£¹ì— ì¶”ê°€
sudo usermod -aG docker $USER

# ë¡œê·¸ì•„ì›ƒ í›„ ì¬ì ‘ì†
exit
ssh -i ~/your-keypair.pem ubuntu@<SERVER1_PUBLIC_IP>

# Docker ì„¤ì¹˜ í™•ì¸
docker --version
docker-compose --version
```

#### Step 3: í”„ë¡œì íŠ¸ í´ë¡ 

```bash
# Git ì„¤ì¹˜
sudo apt-get install -y git

# í”„ë¡œì íŠ¸ í´ë¡ 
cd ~
git clone https://github.com/<your-org>/kafka-event-architecture-lab.git
cd kafka-event-architecture-lab/server1-app
```

#### Step 4: PostgreSQL ì‹¤í–‰

```bash
# Docker ë„¤íŠ¸ì›Œí¬ ìƒì„±
docker network create app-network

# PostgreSQL ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker run -d \
  --name postgres-orders \
  --network app-network \
  -e POSTGRES_USER=orders_user \
  -e POSTGRES_PASSWORD=orders_pass \
  -e POSTGRES_DB=orders_db \
  -p 5432:5432 \
  -v postgres-data:/var/lib/postgresql/data \
  --restart unless-stopped \
  postgres:16-alpine

# PostgreSQL ë¡œê·¸ í™•ì¸
docker logs postgres-orders --tail 20

# PostgreSQL ì—°ê²° í…ŒìŠ¤íŠ¸
docker exec postgres-orders pg_isready -U orders_user -d orders_db
```

#### Step 5: Server1 í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
cd ~/kafka-event-architecture-lab/server1-app

# .env íŒŒì¼ ìƒì„±
cat > .env << EOF
PORT=8080
POSTGRES_HOST=postgres-orders
POSTGRES_PORT=5432
POSTGRES_DB=orders_db
POSTGRES_USER=orders_user
POSTGRES_PASSWORD=orders_pass
KAFKA_BOOTSTRAP_SERVERS=<SERVER2_PRIVATE_IP>:9092
KAFKA_CLIENT_ID=server1-producer
EOF

# Server2ì˜ Private IP í™•ì¸ ë°©ë²•
# Server2 í„°ë¯¸ë„ì—ì„œ: curl -s http://169.254.169.254/latest/meta-data/local-ipv4

# .env íŒŒì¼ í™•ì¸
cat .env
```

#### Step 6: Server1 ì• í”Œë¦¬ì¼€ì´ì…˜ ë¹Œë“œ ë° ì‹¤í–‰

```bash
# Docker ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t server1-app:latest .

# ë¹Œë“œ í™•ì¸
docker images | grep server1-app

# Server1 ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰
docker run -d \
  --name server1-app \
  --network app-network \
  --link postgres-orders \
  -p 8080:8080 \
  --env-file .env \
  --restart unless-stopped \
  server1-app:latest

# ë¡œê·¸ í™•ì¸
docker logs server1-app -f
# Ctrl+Cë¡œ ì¢…ë£Œ
```

**ì •ìƒ ì‹¤í–‰ ë¡œê·¸ ì˜ˆì‹œ:**
```
âœ… Database schema initialized
âœ… Kafka Producer connected
âœ… Kafka Consumer connected and subscribed to order-stats
ğŸš€ Server1 running on http://localhost:8080
ğŸ“Š Dashboard: http://localhost:8080/dashboard
```

#### Step 7: Server1 ìƒíƒœ í™•ì¸

```bash
# ì»¨í…Œì´ë„ˆ ìƒíƒœ í™•ì¸
docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"

# í—¬ìŠ¤ ì²´í¬
curl http://localhost:8080/health
```

**ì˜ˆìƒ ì¶œë ¥:**
```
{"status":"healthy","timestamp":"2025-11-17T15:30:00.123Z"}
```

### ì „ì²´ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸

#### Step 1: ì™¸ë¶€ì—ì„œ ëŒ€ì‹œë³´ë“œ ì ‘ì†

ë¡œì»¬ ë¸Œë¼ìš°ì €ì—ì„œ Server1ì˜ ëŒ€ì‹œë³´ë“œì— ì ‘ì†í•©ë‹ˆë‹¤.

```bash
# ë¸Œë¼ìš°ì €ì—ì„œ ì—´ê¸°
http://<SERVER1_PUBLIC_IP>:8080/dashboard
```

#### Step 2: ì£¼ë¬¸ ìƒì„± í…ŒìŠ¤íŠ¸ (ë¡œì»¬ì—ì„œ)

```bash
# ë‹¨ì¼ ì£¼ë¬¸ ìƒì„±
curl -X POST http://<SERVER1_PUBLIC_IP>:8080/orders \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": "user-001",
    "store_id": "store-777",
    "region": "Seoul",
    "price": 25000
  }'
```

#### Step 3: í†µê³„ í™•ì¸

```bash
# í†µê³„ ì¡°íšŒ
curl http://<SERVER1_PUBLIC_IP>:8080/stats | jq .
```

#### Step 4: ëŒ€ëŸ‰ í…ŒìŠ¤íŠ¸ (Server1ì—ì„œ)

```bash
# Server1 í„°ë¯¸ë„ì—ì„œ
cd ~/kafka-event-architecture-lab

# í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ì— ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
chmod +x test-orders.sh

# 100ê°œ ì£¼ë¬¸ ìƒì„±
SERVER_URL=http://localhost:8080 ./test-orders.sh 100

# ëŒ€ì‹œë³´ë“œì—ì„œ ì‹¤ì‹œê°„ í†µê³„ í™•ì¸
```

#### Step 5: Kafka ë©”ì‹œì§€ í™•ì¸ (Server2ì—ì„œ)

```bash
# Server2 í„°ë¯¸ë„ì—ì„œ
# orders í† í”½ ë©”ì‹œì§€ í™•ì¸
docker exec kafka-broker kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic orders \
  --from-beginning \
  --max-messages 10

# order-stats í† í”½ ë©”ì‹œì§€ í™•ì¸
docker exec kafka-broker kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic order-stats \
  --from-beginning \
  --max-messages 10
```

### ëª¨ë‹ˆí„°ë§ ë° ê´€ë¦¬

#### ì»¨í…Œì´ë„ˆ ë¡œê·¸ í™•ì¸

```bash
# Server1ì—ì„œ
docker logs server1-app --tail 100 -f

# Server2ì—ì„œ
docker logs server2-streams --tail 100 -f
docker logs kafka-broker --tail 100 -f
```

#### ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘

```bash
# Server1 ì¬ì‹œì‘
docker restart server1-app

# Server2 ì¬ì‹œì‘
docker restart server2-streams

# Kafka ì¬ì‹œì‘
docker restart kafka-broker

# PostgreSQL ì¬ì‹œì‘
docker restart postgres-orders
```

#### ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ í™•ì¸

```bash
# ì»¨í…Œì´ë„ˆë³„ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰
docker stats

# ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰
docker system df

# ë„¤íŠ¸ì›Œí¬ í™•ì¸
docker network inspect kafka-network  # Server2ì—ì„œ
docker network inspect app-network    # Server1ì—ì„œ
```

### ë°°í¬ ë¬¸ì œ í•´ê²°

#### Kafka ì—°ê²° ì‹¤íŒ¨

**ì¦ìƒ:** Server1ì—ì„œ Kafka ì—°ê²° ì˜¤ë¥˜ ë°œìƒ

```bash
# Server1ì—ì„œ Server2ë¡œ ì—°ê²° í…ŒìŠ¤íŠ¸
telnet <SERVER2_PRIVATE_IP> 9092
# ë˜ëŠ”
nc -zv <SERVER2_PRIVATE_IP> 9092
```

**í•´ê²° ë°©ë²•:**
1. Server2 ë³´ì•ˆ ê·¸ë£¹ì—ì„œ Server1ì˜ ë³´ì•ˆ ê·¸ë£¹ì´ 9092 í¬íŠ¸ ì ‘ê·¼ í—ˆìš©ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
2. Kafka ADVERTISED_LISTENERS ì„¤ì • í™•ì¸
3. Kafka ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘

#### PostgreSQL ì—°ê²° ì‹¤íŒ¨

```bash
# Server1ì—ì„œ PostgreSQL ì—°ê²° í…ŒìŠ¤íŠ¸
docker exec server1-app ping postgres-orders

# PostgreSQL ë¡œê·¸ í™•ì¸
docker logs postgres-orders --tail 50
```

#### í¬íŠ¸ ì´ë¯¸ ì‚¬ìš© ì¤‘

```bash
# í¬íŠ¸ ì‚¬ìš© ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ í™•ì¸
sudo lsof -i :8080  # Server1
sudo lsof -i :9092  # Server2

# í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
sudo kill -9 <PID>
```

### ì‹œìŠ¤í…œ ì¢…ë£Œ ë° ì •ë¦¬

#### Server1 ì •ë¦¬

```bash
# ì»¨í…Œì´ë„ˆ ì¤‘ì§€ ë° ì‚­ì œ
docker stop server1-app postgres-orders
docker rm server1-app postgres-orders

# ë³¼ë¥¨ ì‚­ì œ (ë°ì´í„° ì˜êµ¬ ì‚­ì œ)
docker volume rm postgres-data

# ë„¤íŠ¸ì›Œí¬ ì‚­ì œ
docker network rm app-network

# ì´ë¯¸ì§€ ì‚­ì œ
docker rmi server1-app:latest
```

#### Server2 ì •ë¦¬

```bash
# ì»¨í…Œì´ë„ˆ ì¤‘ì§€ ë° ì‚­ì œ
docker stop server2-streams kafka-broker zookeeper
docker rm server2-streams kafka-broker zookeeper

# ë„¤íŠ¸ì›Œí¬ ì‚­ì œ
docker network rm kafka-network

# ì´ë¯¸ì§€ ì‚­ì œ
docker rmi server2-streams:latest
```

---

## API ëª…ì„¸

### Server1 REST API

#### 1. í—¬ìŠ¤ ì²´í¬

```http
GET /health
```

**Response 200:**
```json
{
  "status": "healthy",
  "timestamp": "2025-11-17T15:30:00.123Z"
}
```

#### 2. ì£¼ë¬¸ ìƒì„±

```http
POST /orders
Content-Type: application/json
```

**Request Body:**
```json
{
  "user_id": "user-001",
  "store_id": "store-777",
  "region": "Seoul",
  "price": 25000
}
```

**Response 201:**
```json
{
  "success": true,
  "order": {
    "order_id": "a1b2c3d4-e5f6-g7h8-i9j0-k1l2m3n4o5p6",
    "user_id": "user-001",
    "store_id": "store-777",
    "region": "Seoul",
    "price": 25000,
    "status": "CREATED",
    "created_at": "2025-11-17T15:30:00.123Z"
  }
}
```

**Response 400 (Bad Request):**
```json
{
  "error": "Missing required fields: user_id, store_id, region, price"
}
```

#### 3. ì£¼ë¬¸ ëª©ë¡ ì¡°íšŒ

```http
GET /orders?limit=100
```

**Query Parameters:**
- `limit` (optional): ì¡°íšŒí•  ì£¼ë¬¸ ìˆ˜ (ê¸°ë³¸ê°’: 100)

**Response 200:**
```json
{
  "success": true,
  "count": 10,
  "orders": [
    {
      "order_id": "...",
      "user_id": "user-001",
      "store_id": "store-777",
      "region": "Seoul",
      "price": 25000,
      "status": "CREATED",
      "created_at": "2025-11-17T15:30:00.123Z"
    }
  ]
}
```

#### 4. í†µê³„ ì¡°íšŒ

```http
GET /stats
```

**Response 200:**
```json
{
  "success": true,
  "stats": {
    "totalOrders": 100,
    "totalSales": 2500000,
    "byRegion": {
      "Seoul": {
        "orders": 50,
        "sales": 1250000
      },
      "Busan": {
        "orders": 30,
        "sales": 750000
      },
      "Incheon": {
        "orders": 20,
        "sales": 500000
      }
    },
    "lastUpdated": "2025-11-17T15:30:00.123Z"
  }
}
```

#### 5. ëŒ€ì‹œë³´ë“œ UI

```http
GET /dashboard
```

HTML í˜ì´ì§€ ë°˜í™˜

---

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 1: Kafka ì—°ê²° ì‹œê°„ ì´ˆê³¼

**ì¦ìƒ:**
```
Error: Connection timeout for kafka:9092
```

**ì›ì¸:**
- Kafka ë¸Œë¡œì»¤ê°€ ì•„ì§ ì‹œì‘ ì¤‘
- ì˜ëª»ëœ Kafka ì£¼ì†Œ ì„¤ì •
- ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ

**í•´ê²°:**
```bash
# 1. Kafka ìƒíƒœ í™•ì¸
docker logs kafka-broker --tail 50

# 2. Kafkaê°€ ì™„ì „íˆ ì‹œì‘ë  ë•Œê¹Œì§€ ëŒ€ê¸° (ì•½ 30ì´ˆ)

# 3. Kafka ì—°ê²° í…ŒìŠ¤íŠ¸
docker exec kafka-broker kafka-broker-api-versions \
  --bootstrap-server localhost:9092

# 4. Server1 ì¬ì‹œì‘
docker restart server1-app
```

### ë¬¸ì œ 2: PostgreSQL ì´ˆê¸°í™” ì‹¤íŒ¨

**ì¦ìƒ:**
```
Error: relation "orders" does not exist
```

**ì›ì¸:**
- ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆê°€ ìƒì„±ë˜ì§€ ì•ŠìŒ
- PostgreSQL ì—°ê²° ì‹¤íŒ¨

**í•´ê²°:**
```bash
# 1. PostgreSQL ìƒíƒœ í™•ì¸
docker logs postgres-orders --tail 50

# 2. PostgreSQL ì ‘ì†í•˜ì—¬ ìˆ˜ë™ í…Œì´ë¸” ìƒì„±
docker exec -it postgres-orders psql -U orders_user -d orders_db

# SQL ì‹¤í–‰
CREATE TABLE IF NOT EXISTS orders (
  order_id VARCHAR(255) PRIMARY KEY,
  user_id VARCHAR(255) NOT NULL,
  store_id VARCHAR(255) NOT NULL,
  region VARCHAR(100) NOT NULL,
  price DECIMAL(10, 2) NOT NULL,
  status VARCHAR(50) NOT NULL,
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

\q  # ì¢…ë£Œ

# 3. Server1 ì¬ì‹œì‘
docker restart server1-app
```

### ë¬¸ì œ 3: Kafka Streams ì¬ì‹œì‘ í›„ ì¤‘ë³µ ì²˜ë¦¬

**ì¦ìƒ:**
í†µê³„ê°€ ê°‘ìê¸° 2ë°°ë¡œ ì¦ê°€

**ì›ì¸:**
Kafka Streams ì• í”Œë¦¬ì¼€ì´ì…˜ì´ í† í”½ì„ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì½ìŒ

**í•´ê²°:**
```bash
# Kafka Streams ìƒíƒœ ì €ì¥ì†Œ ì‚­ì œ
docker exec kafka-broker kafka-streams-application-reset \
  --application-id order-stats-app \
  --input-topics orders \
  --bootstrap-servers localhost:9092

# Server2 ì¬ì‹œì‘
docker restart server2-streams
```

### ë¬¸ì œ 4: Server2 (Kafka Streams) ì¬ì‹œì‘ ë°˜ë³µ

**ì¦ìƒ:**
```
Server2 ì»¨í…Œì´ë„ˆê°€ ê³„ì† ì¬ì‹œì‘ë¨
ë¡œê·¸ì— "UNKNOWN_TOPIC_OR_PARTITION" ì—ëŸ¬ ë°˜ë³µ
```

**ì›ì¸:**
- ë‹¨ì¼ ë¸Œë¡œì»¤ í™˜ê²½ì—ì„œ replication factor ê¸°ë³¸ê°’(3)ì´ ì„¤ì •ë˜ì–´ ìˆìŒ
- Kafka Streams ë‚´ë¶€ í† í”½(repartition, changelog) ìƒì„± ì‹¤íŒ¨

**í•´ê²°:**
ì´ ë¬¸ì œëŠ” ì´ë¯¸ ì½”ë“œì— ë°˜ì˜ë˜ì–´ ìˆì§€ë§Œ, ë§Œì•½ ë°œìƒí•œë‹¤ë©´:

```java
// server2-streams/src/main/java/com/example/streams/OrderStatsApp.javaì— ì¶”ê°€
props.put(StreamsConfig.REPLICATION_FACTOR_CONFIG, 1);
props.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 0);
```

ê·¸ë¦¬ê³  Server2 ì¬ë¹Œë“œ:
```bash
cd infra-local
docker-compose stop server2
docker-compose build server2
docker-compose up -d server2
```

### ë¬¸ì œ 5: í¬íŠ¸ ì¶©ëŒ

**ì¦ìƒ:**
```
Error: Port 8080 is already in use
```

**í•´ê²°:**
```bash
# 1. í¬íŠ¸ ì‚¬ìš© ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ í™•ì¸
sudo lsof -i :8080

# 2. í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
sudo kill -9 <PID>

# 3. ë˜ëŠ” ë‹¤ë¥¸ í¬íŠ¸ ì‚¬ìš©
docker run -d \
  --name server1-app \
  -p 8081:8080 \
  ...
```

### ë¬¸ì œ 5: ì»¨í…Œì´ë„ˆ ë©”ëª¨ë¦¬ ë¶€ì¡±

**ì¦ìƒ:**
```
docker: Error response from daemon: OOM command not allowed when used memory > 'maxmemory'.
```

**í•´ê²°:**
```bash
# 1. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
docker stats

# 2. ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì •
docker run -d \
  --name server2-streams \
  --memory="2g" \
  --memory-swap="2g" \
  ...

# 3. ë¶ˆí•„ìš”í•œ ì»¨í…Œì´ë„ˆ ì •ë¦¬
docker system prune -a
```

### ë¬¸ì œ 6: ëŒ€ì‹œë³´ë“œì— í†µê³„ê°€ í‘œì‹œë˜ì§€ ì•ŠìŒ

**ì›ì¸:**
- Kafka Consumerê°€ order-stats í† í”½ì„ êµ¬ë…í•˜ì§€ ëª»í•¨
- Server2 Streamsê°€ í†µê³„ë¥¼ ë°œí–‰í•˜ì§€ ì•ŠìŒ

**í•´ê²°:**
```bash
# 1. Server1 ë¡œê·¸ í™•ì¸
docker logs server1-app | grep "order-stats"

# 2. Server2 ë¡œê·¸ í™•ì¸
docker logs server2-streams | grep "Publishing stats"

# 3. order-stats í† í”½ì— ë©”ì‹œì§€ê°€ ìˆëŠ”ì§€ í™•ì¸
docker exec kafka-broker kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic order-stats \
  --from-beginning \
  --max-messages 5

# 4. ì£¼ë¬¸ì„ ìƒˆë¡œ ìƒì„±í•˜ì—¬ ì´ë²¤íŠ¸ ë°œìƒì‹œí‚¤ê¸°
curl -X POST http://localhost:8080/orders \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": "test",
    "store_id": "test",
    "region": "Seoul",
    "price": 10000
  }'
```

---

## ì¶”ê°€ í•™ìŠµ ìë£Œ

### Kafka Streams ê°œë…

ì´ í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš©ëœ Kafka Streams í•µì‹¬ ê°œë…:

1. **KStream**: ë¬´í•œí•œ ë ˆì½”ë“œ ìŠ¤íŠ¸ë¦¼
2. **KTable**: ë³€ê²½ ê°€ëŠ¥í•œ ìƒíƒœ í…Œì´ë¸”
3. **Aggregation**: ë°ì´í„° ì§‘ê³„ (sum, count ë“±)
4. **Grouping**: í‚¤ ê¸°ë°˜ ê·¸ë£¹í•‘
5. **Stateful Processing**: ìƒíƒœ ì €ì¥ ì²˜ë¦¬

### ì´ë²¤íŠ¸ ê¸°ë°˜ ì•„í‚¤í…ì²˜ ì¥ì 

1. **ëŠìŠ¨í•œ ê²°í•©**: ì„œë¹„ìŠ¤ ê°„ ë…ë¦½ì„±
2. **í™•ì¥ì„±**: ìˆ˜í‰ì  í™•ì¥ ìš©ì´
3. **ë¹„ë™ê¸° ì²˜ë¦¬**: ì‘ë‹µ ì‹œê°„ ê°œì„ 
4. **ì¥ì•  ê²©ë¦¬**: í•œ ì„œë¹„ìŠ¤ ì¥ì• ê°€ ì „ì²´ì— ì˜í–¥ X
5. **ì´ë²¤íŠ¸ ì†Œì‹±**: ëª¨ë“  ë³€ê²½ ì´ë ¥ ì¶”ì  ê°€ëŠ¥

### ì‹¤ìŠµ í™•ì¥ ì•„ì´ë””ì–´

1. **ì§€ì†ì„± í–¥ìƒ**:
   - RocksDB State Store ì‚¬ìš©
   - Kafka Connectë¡œ DB ë™ê¸°í™”

2. **ëª¨ë‹ˆí„°ë§ ì¶”ê°€**:
   - Prometheus + Grafana
   - Kafka JMX Metrics
   - APM ë„êµ¬ ì—°ë™

3. **ê³ ê°€ìš©ì„± êµ¬ì„±**:
   - Kafka í´ëŸ¬ìŠ¤í„° (3+ ë¸Œë¡œì»¤)
   - Replication Factor ì„¤ì •
   - Multi-AZ ë°°í¬

4. **ë³´ì•ˆ ê°•í™”**:
   - Kafka SSL/TLS ì•”í˜¸í™”
   - SASL ì¸ì¦
   - API ì¸ì¦/ì¸ê°€

---

## ë¼ì´ì„¼ìŠ¤

MIT License

---

## ë¬¸ì˜

ì´ìŠˆ ë˜ëŠ” ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ GitHub Issuesë¥¼ í†µí•´ ë¬¸ì˜í•´ ì£¼ì„¸ìš”.

**ì œì‘ì:** Kafka Event Architecture Lab Team
**ë²„ì „:** 1.0.0
**ìµœì¢… ì—…ë°ì´íŠ¸:** 2025-11-17
